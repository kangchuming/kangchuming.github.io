# RAG 准确率评估报告

## 一、项目背景与目标

### 1. 项目背景（Situation）
- 项目初期，生成内容常出现专业术语错误
- 医疗领域术语混淆问题突出
- 需要通过 RAG 增强生成准确性

### 2. 项目目标（Task）
- 设计量化指标
- 对比基线模型（纯大模型生成）与 RAG 方案的效果差异

## 二、实施过程（Action）

### 1. 基线定义
- 使用纯星火大模型生成内容
- 人工标注 1000 条测试集
- 统计术语准确率（正确术语数/总术语数）
- 初始准确率：72%

### 2. RAG 优化方案
- 通过 LangChain 串联 Milvus 向量检索
- 使用 Hugging Face text2vec 编码
- 从企业知识库检索 Top3 相关文档片段作为上下文

### 3. 评估方法
- 相同测试集下，RAG 方案准确率提升至 92%
- 绝对值提升：20%
- 相对提升：28%（20/72≈27.8%）
- 因业务场景复杂度，保守表述为"提升 28%"

### 4. 项目成果（Result）
- 准确率提升显著
- 客户验收通过率从 65% 提升至 89%

## 三、量化方法严谨性分析

### 1. 准确率提升表述
#### Q1: 准确率绝对值提升 20%（72%→92%），为何汇报时表述为"28%提升"？
- "28%"是相对提升率（20/72≈27.8%）
- 采用相对值更能体现技术优化对原始短板的改进幅度
- 汇报时明确标注了两种指标
- 因业务方更关注"效率增益比例"，最终选用相对值
- 已向客户同步说明两种计算方式，无误导意图

### 2. 测试集规模与统计显著性
#### Q2: 测试集仅 1000 条样本是否足够？
- 1000 条样本覆盖了 15 类高频医学术语场景
- 经卡方检验(p<0.01)确认差异显著
- 采用交叉验证，分割 5 个子集计算置信区间（89%-94%）
- 结果稳定性达标

## 四、基线对比分析

### 1. 基线模型选择
#### Q3: 基线模型选择是否公平？
- 基线模型与 RAG 方案的生成器均为星火大模型
- 控制变量单一（仅追加检索模块）
- 客户现有技术栈依赖星火
- 对比 GPT-4 不符合实际业务成本约束
- 脱离项目目标（验证 RAG 增益，非模型对比）

### 2. 基线模型性能
#### Q4: 基线准确率仅 72%，是否存在模型未充分微调的问题？
- 基线模型已针对医疗文本进行 Lora 微调
- 专业术语长尾分布（如"心肌缺血"vs"心肌梗死"）
- 纯生成模型易混淆相近术语
- RAG 通过引入知识库检索，直接补充细粒度概念定义

## 五、技术实现细节

### 1. 检索策略
#### Q5: 为何选择 Top3 文档片段？
- 经实验验证，Top3 时准确率饱和（Top5 仅+0.7%）
- 更长上下文会引入无关噪声
- 检索模块使用 Milvus 的欧氏距离排序
- 设置阈值过滤相似度<0.8 的片段，确保有效性

### 2. 标注质量
#### Q6: 人工标注的术语正确性如何保证？
- 标注由 3 名医学背景标注员独立完成
- 使用标准化术语库（ICD-10、SNOMED CT）作为参考答案
- 争议案例由主治医师仲裁
- 最终 Kappa 系数达 0.91，一致性优秀